{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam 2563 - Neural Network (Orthopedic Patients Problem)\n",
    "\n",
    "This exam problem has an objective to develop a neural network model to classify patients as belonging to one out of three categories: Normal (0), Disk Hernia (1) or Spondylolisthesis (2) from six biomechanical attributes (features) derived from the shape and orientation of the pelvis and lumbar spine:\n",
    "\n",
    "1. pelvic incidence\n",
    "2. pelvic tilt\n",
    "3. lumbar lordosis angle\n",
    "4. sacral slope\n",
    "5. pelvic radius\n",
    "6. grade of spondylolisthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# library written for this exam\n",
    "import utilsNN as utils\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We start this exam problem by first loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset and test dataset\n",
    "\n",
    "# Read tab separated data\n",
    "data_train = np.loadtxt(os.path.join('Data', 'NNOrthopedicPatientsData_training.txt'))\n",
    "data_test = np.loadtxt(os.path.join('Data', 'NNOrthopedicPatientsData_test.txt'))\n",
    "\n",
    "# First six columns of data are features and the last column is the label.\n",
    "# Matrix X contains six features while vector y contains the label.\n",
    "\n",
    "X, y = data_train[:, 0:6], data_train[:, 6].astype(int)\n",
    "X_test, y_test = data_test[:, 0:6], data_test[:, 6].astype(int)\n",
    "\n",
    "m = y.size  # number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have been provided with a set of initialized network parameters ($\\Theta^{(1)}, \\Theta^{(2)}$). These are stored in `SampleOrthopedicWeight1.txt` and `SampleOrthopedicWeight2.txt` which will be loaded in the next cell of this notebook into `Theta1` and `Theta2`. The parameters have dimensions that are sized for a neural network with **20 hidden units** in the second layer (hidden layer) and **3 output units** (corresponding to three patient categories) in the third layer (output layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Theta1 = (20, 7)\n",
      "Shape of Theta2 = (3, 21)\n"
     ]
    }
   ],
   "source": [
    "# Load initiallized network parameters\n",
    "\n",
    "Theta1 = np.loadtxt(os.path.join('Data', 'SampleOrthopedicWeight1.txt'))\n",
    "Theta2 = np.loadtxt(os.path.join('Data', 'SampleOrthopedicWeight2.txt'))\n",
    "print('Shape of Theta1 =', Theta1.shape)\n",
    "print('Shape of Theta2 =', Theta2.shape)\n",
    "\n",
    "# Unroll parameters \n",
    "# To unroll the matrix into vector (1-D array), we use `np.ravel()` \n",
    "nn_params = np.concatenate([np.ravel(Theta1), np.ravel(Theta2)])\n",
    "initial_nn_params = nn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.16380949e-01,  6.98418999e-02,  2.45563573e-02,  1.01284469e-01,\n",
       "       -8.71080718e-02,  8.36715528e-02, -6.33857537e-02,  4.63551132e-02,\n",
       "       -8.20442980e-02, -7.17665089e-02,  6.48633031e-02, -6.83272813e-02,\n",
       "       -2.21245953e-02, -7.60535483e-02, -9.82373545e-03,  3.38100964e-02,\n",
       "        1.08184098e-02, -1.15067957e-02, -4.64111014e-02, -6.65563225e-02,\n",
       "       -6.80657400e-02, -9.20714560e-02, -3.74703159e-02,  1.39646638e-02,\n",
       "       -5.32309366e-02,  1.12973174e-01, -4.29636647e-03,  9.15016378e-02,\n",
       "       -4.58161410e-02, -4.83393539e-02,  2.36853054e-02,  6.78081526e-02,\n",
       "        5.91726874e-02,  5.63878720e-02, -1.12240952e-02,  2.21384164e-03,\n",
       "       -6.56123521e-02,  1.04366735e-01, -9.61648508e-03, -4.01843327e-02,\n",
       "        1.89899690e-02, -9.14676085e-02, -8.88336797e-02,  1.11441885e-01,\n",
       "       -5.55739520e-02, -3.87953098e-02,  1.04044084e-01,  9.15493388e-02,\n",
       "       -1.11053344e-01, -9.92191618e-02, -8.75259371e-02,  3.02835335e-02,\n",
       "        3.79152561e-02,  5.08059718e-02, -1.68515894e-02,  8.61663267e-02,\n",
       "        5.56030494e-02,  6.29715077e-02, -1.04684717e-01,  1.82269043e-02,\n",
       "        4.10775975e-02, -4.26541891e-02,  8.88352471e-02,  6.58876537e-02,\n",
       "       -2.41997863e-02,  1.99597008e-02,  1.10232055e-01, -2.97771915e-02,\n",
       "       -2.84367511e-02, -1.74277781e-03,  1.11990570e-01,  3.71155293e-02,\n",
       "       -1.50067391e-02, -6.58584011e-02,  3.53640875e-02,  7.56682965e-02,\n",
       "       -4.69245549e-05,  8.02950612e-02, -4.76545852e-02, -6.67024173e-02,\n",
       "        1.41492558e-02,  9.89570988e-02, -8.66166131e-02, -6.93771577e-02,\n",
       "       -9.13718535e-03, -2.38785672e-02,  5.90045136e-02,  7.78601305e-02,\n",
       "        3.69049457e-02,  4.91889625e-02,  4.51562893e-02,  2.90771208e-02,\n",
       "        6.14927790e-02, -8.33577253e-02, -7.53320347e-02,  1.06355746e-01,\n",
       "        1.03355123e-04,  9.88665684e-02,  3.74987504e-02, -6.80665037e-02,\n",
       "       -9.35826103e-02, -3.95896300e-02, -2.36778366e-02,  1.10496793e-01,\n",
       "        5.37658475e-02, -3.68613785e-02,  9.02367375e-02,  9.40790041e-02,\n",
       "       -4.96881126e-02,  5.77169072e-02, -2.47974706e-02,  1.64310211e-02,\n",
       "        1.59565855e-02, -8.36521089e-02,  7.60351416e-02, -8.96539905e-02,\n",
       "       -1.72721647e-02,  5.02230702e-03,  3.25703524e-02,  8.42923604e-02,\n",
       "       -1.04241433e-01,  8.96454496e-02, -2.60765387e-02,  6.78843729e-02,\n",
       "        5.52704130e-02, -1.03223068e-01, -6.27972718e-02, -3.70792384e-02,\n",
       "        1.04348026e-02,  5.12367100e-02,  5.98340129e-02, -9.69936084e-02,\n",
       "       -3.33097481e-02,  2.26757442e-02, -5.96912380e-04, -8.94716310e-02,\n",
       "       -5.08267721e-03,  6.45824181e-02,  1.12900185e-01, -9.15673753e-02,\n",
       "       -6.11866436e-02, -9.00416449e-02, -9.67578014e-03, -2.45561546e-02,\n",
       "        6.75459279e-02, -2.50349249e-02, -1.11847839e-02, -2.91166916e-02,\n",
       "        2.59134793e-02, -3.73942515e-02,  8.96648678e-02, -1.14149479e-01,\n",
       "        1.19456421e-01,  1.06457982e-01, -1.05758331e-01,  4.66461451e-02,\n",
       "        1.29339918e-02,  8.06799978e-02, -1.03646068e-01,  1.39966665e-02,\n",
       "        1.43960742e-02,  3.41613472e-02,  5.66894729e-02, -3.29506641e-02,\n",
       "       -3.55489869e-02, -5.56326195e-02, -1.15208841e-01,  8.93690644e-02,\n",
       "        9.01173432e-02,  1.09277139e-01,  4.00153446e-02, -7.84502724e-02,\n",
       "        4.04083374e-02,  4.20578841e-02, -1.03655174e-01,  1.12609239e-02,\n",
       "       -1.03564441e-01,  4.32308247e-02, -1.16882918e-01, -6.51714351e-02,\n",
       "        3.78216984e-02,  9.62691128e-02, -1.74351254e-02, -6.66824476e-02,\n",
       "        7.49307659e-03,  9.55215524e-02, -8.44304660e-02, -1.94207853e-02,\n",
       "        7.75603616e-02, -6.82128443e-02, -5.00107505e-02,  3.54209908e-02,\n",
       "        1.87024112e-02,  9.57030247e-02, -4.69148945e-02,  4.22632718e-02,\n",
       "       -1.93016073e-02, -1.11789120e-01,  7.04655713e-03,  9.79111961e-03,\n",
       "       -9.17997718e-02, -5.58694465e-02,  1.93296118e-02])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model representation\n",
    "\n",
    "The neural network has 3 layers - an input layer, a hidden layer and an output layer. \n",
    "\n",
    "The inputs are 6 biomechanical attributes. \n",
    "\n",
    "The hidden layer has 15 neurons.\n",
    "\n",
    "The outputs are 3 patient categories (0 to 2).\n",
    "\n",
    "The training data was loaded into the variables `X` and `y` while the test data was loaded into the variables `X_test` and `y_test` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the parameters you will use for this exam problem by yourself!\n",
    "input_layer_size  = 6\n",
    "hidden_layer_size = 20\n",
    "num_labels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute sigmoid function given the input z.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        The input to the sigmoid function. This can be a 1-D vector \n",
    "        or a 2-D matrix. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    g : array_like\n",
    "        The computed sigmoid function. g has the same shape as z, since\n",
    "        the sigmoid is computed element-wise on z.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the sigmoid of each value of z (z can be a matrix, vector or scalar).\n",
    "    \"\"\"\n",
    "    # convert input to a numpy array\n",
    "    z = np.array(z)\n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the sigmoid function evaluated at z. \n",
    "    This should work regardless if z is a matrix or a vector. \n",
    "    In particular, if z is a vector or matrix, you should return\n",
    "    the gradient for each element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        A vector or matrix as input to the sigmoid function. \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    g : array_like\n",
    "        Gradient of the sigmoid function. Has the same shape as z. \n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the gradient of the sigmoid function evaluated at\n",
    "    each value of z (z can be a matrix, vector or scalar).    \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    g = sigmoid(z) * (1 - sigmoid(z))\n",
    "    # ============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_=0.0):\n",
    "    \n",
    "    \n",
    "\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))  # dimension = 25 X 401\n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))  # dimension = 10 X 26\n",
    "    # Setup some useful variables\n",
    "    m = y.size\n",
    "         \n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    grad = []\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "\n",
    "    \n",
    "    # ====================== Code for Section 1.3 Feedforward and cost function ======================\n",
    "    a1 = np.concatenate([np.ones((m,1)),X],axis =1 ) # dimension = 5000 X 401\n",
    "    \n",
    "    z2 = np.dot(a1,Theta1.T) # dimension = 5000 X 25\n",
    "    a2 = sigmoid(z2) # dimension = 5000 X 25\n",
    "    # Add bias unit to a2\n",
    "    a2 = np.concatenate([np.ones((a2.shape[0],1)),a2],axis=1) # 5000 X 26\n",
    "    \n",
    "    z3 = np.dot(a2,Theta2.T) # dimension = 5000 X 10\n",
    "    a3 = sigmoid(z3) # dimension = 5000 X 10\n",
    "    \n",
    "    # values of the hypothesis h= activation unit lalues of the last (output) Layer\n",
    "    h = a3 # dimension = 5000 X 10\n",
    "    \n",
    "    ### Cost Function ###\n",
    "    #set y_matrix to be a matrix of (m x num_labels) where the element in each row that equals to 1 will be the \n",
    "    #(value of y + 1 ) th element in that row, for example, if the value of y in the 10th row = 2, y[9] = 2, then\n",
    "    # the (2+1 = 3)rd element of the 10th row will be 1 while other elements are zeros.\n",
    "    #i.e. [0., 0., 1., 0.,0.,0.,0.,0.,0.,0.]\n",
    "    y_matrix = y\n",
    "    y_matrix = np.eye(num_labels)[y_matrix] # dimension = 5000 X 10\n",
    "    \n",
    "    J = (-1 / m ) * np.sum((np.log(h) * y_matrix) + np.log(1 - h) * (1 - y_matrix))\n",
    "\n",
    "    # ====================== End of Code for Section 1.3 Feedforward and cost function ======================\n",
    "    #!!\n",
    "    #!!\n",
    "    # ====================== Code for Section 1.4 Regularized cost function ======================\n",
    "    # Add regularization term to the cost function from Section 1.3\n",
    "    # reg_term does not include the square of theta of bias units, \n",
    "    # For the matrices Theta1 and Theta2, this corresponds to the first column of each matrix.\n",
    "    # Therefore we use [:, 1:] to square every theta except the first column of the matrix \n",
    "    # (the theta of bias units)\n",
    "    \n",
    "    reg_term = (lambda_ / (2*m)) * (np.sum(np.square(Theta1[:, 1:])) + np.sum(np.square(Theta2[:, 1:])))\n",
    "    J = J + reg_term\n",
    "    \n",
    "    # ====================== End of Code for Section 1.4 Regularized cost function ======================\n",
    "    #!!\n",
    "    #!!                 \n",
    "    # ====================== Code for Section 2.3 Backpropagation ======================\n",
    "    delta_3 = h - y_matrix # dimension = 5000 X 10\n",
    "    \n",
    "    # To calculate delta_2, do not include weight form bias unit, therefore we use  Theta2[:, 1:]\n",
    "    # (Not include the first column of Theta2 which corresponds to the weigth of bias unit)\n",
    "    \n",
    "    delta_2 = np.dot(delta_3, Theta2[:, 1:]) * sigmoidGradient(z2) # dimension =500 X 25\n",
    "    \n",
    "    Delta1 = np.dot(delta_2.T,a1) # dimension = 25 X 401 ---> The first column of Delta1 corresponds to the bias units\n",
    "    Delta2 = np.dot(delta_3.T,a2) # dimension = 10 X 26 ---> The first column of Delta2 corresponds to the bias units\n",
    "    \n",
    "    Theta1_grad = (1/m)*Delta1 # dimension = 25 X 401\n",
    "    Theta2_grad = (1/m)*Delta2 # dimension = 10 X 26\n",
    "    \n",
    "    # To untoll the matrix into vector (1-D array), we use np.ravel()\n",
    "    \n",
    "    grad = np.concatenate([np.ravel(Theta1_grad), np.ravel(Theta2_grad)])\n",
    "    \n",
    "    # ====================== End of Code for Section 2.3 Backpropagation ======================\n",
    "    #!!\n",
    "    #!!\n",
    "    # ====================== Code for Section 2.5 Regularized Neural Network ======================\n",
    "    \n",
    "    # Add regularization to gradient\n",
    "    Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (lambda_ / m) * Theta1[:, 1:]\n",
    "    Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (lambda_ / m) * Theta2[:, 1:]\n",
    "    \n",
    "    # update grad with regularizatoin\n",
    "    grad = np.concatenate([np.ravel(Theta1_grad), np.ravel(Theta2_grad)]) \n",
    "    # ====================== End of Code for Section 2.5 Regularized Neural Network ======================\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights): 2.068831\n",
      "Cost at parameters (loaded from ex4weights): 2.070556\n"
     ]
    }
   ],
   "source": [
    "# Weight regularization parameter (we set this to 1 here).\n",
    "lambda_ = 0\n",
    "J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                      num_labels, X, y, lambda_)\n",
    "\n",
    "print('Cost at parameters (loaded from ex4weights): %.6f' % J)\n",
    "\n",
    "lambda_ = 1\n",
    "J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                      num_labels, X, y, lambda_)\n",
    "\n",
    "print('Cost at parameters (loaded from ex4weights): %.6f' % J)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 21.739130\n"
     ]
    }
   ],
   "source": [
    "pred = utils.predict(Theta1, Theta2, X)\n",
    "print('Training Set Accuracy: %f' % (np.mean(pred == y) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 12.500000\n"
     ]
    }
   ],
   "source": [
    "pred = utils.predict(Theta1, Theta2, X_test)\n",
    "print('Testing Set Accuracy: %f' % (np.mean(pred == y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "options= {'maxiter': 2000}\n",
    "\n",
    "lambda_ = 0\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument\n",
    "# (the neural network parameters)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# get the solution of the optimization\n",
    "nn_params_Final = res.x\n",
    "        \n",
    "# Obtain the optimal Theta1 and Theta2 back from nn_params\n",
    "Theta1_Final = np.reshape(nn_params_Final[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "Theta2_Final = np.reshape(nn_params_Final[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.27825235e-03 -9.27825236e-03]\n",
      " [-3.04978709e-06 -3.04978914e-06]\n",
      " [-1.75060084e-04 -1.75060082e-04]\n",
      " [-9.62660640e-05 -9.62660620e-05]\n",
      " [ 8.89911959e-03  8.89911960e-03]\n",
      " [ 1.42869450e-05  1.42869443e-05]\n",
      " [ 2.33146358e-04  2.33146357e-04]\n",
      " [ 1.17982666e-04  1.17982666e-04]\n",
      " [-8.36010761e-03 -8.36010762e-03]\n",
      " [-2.59383093e-05 -2.59383100e-05]\n",
      " [-2.87468729e-04 -2.87468729e-04]\n",
      " [-1.37149709e-04 -1.37149706e-04]\n",
      " [ 7.62813550e-03  7.62813551e-03]\n",
      " [ 3.69883257e-05  3.69883234e-05]\n",
      " [ 3.35320351e-04  3.35320347e-04]\n",
      " [ 1.53247082e-04  1.53247082e-04]\n",
      " [-6.74798369e-03 -6.74798370e-03]\n",
      " [-4.68759764e-05 -4.68759769e-05]\n",
      " [-3.76215583e-04 -3.76215587e-04]\n",
      " [-1.66560294e-04 -1.66560294e-04]\n",
      " [ 3.14544970e-01  3.14544970e-01]\n",
      " [ 1.64090819e-01  1.64090819e-01]\n",
      " [ 1.64567932e-01  1.64567932e-01]\n",
      " [ 1.58339334e-01  1.58339334e-01]\n",
      " [ 1.51127527e-01  1.51127527e-01]\n",
      " [ 1.49568335e-01  1.49568335e-01]\n",
      " [ 1.11056588e-01  1.11056588e-01]\n",
      " [ 5.75736494e-02  5.75736493e-02]\n",
      " [ 5.77867378e-02  5.77867378e-02]\n",
      " [ 5.59235296e-02  5.59235296e-02]\n",
      " [ 5.36967009e-02  5.36967009e-02]\n",
      " [ 5.31542052e-02  5.31542052e-02]\n",
      " [ 9.74006970e-02  9.74006970e-02]\n",
      " [ 5.04575855e-02  5.04575855e-02]\n",
      " [ 5.07530173e-02  5.07530173e-02]\n",
      " [ 4.91620841e-02  4.91620841e-02]\n",
      " [ 4.71456249e-02  4.71456249e-02]\n",
      " [ 4.65597186e-02  4.65597186e-02]]\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 2.41486e-11\n",
      "\n",
      "\n",
      "Cost at (fixed) debugging parameters (w/ lambda = 0.000000): 0.371930 \n"
     ]
    }
   ],
   "source": [
    "utils.checkNNGradients(nnCostFunction, lambda_)\n",
    "\n",
    "# Also output the costFunction debugging values\n",
    "debug_J, _  = nnCostFunction(nn_params_Final, input_layer_size,\n",
    "                          hidden_layer_size, num_labels, X, y, lambda_)\n",
    "\n",
    "print('\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): %f ' % (lambda_, debug_J))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "options= {'maxiter': 2000}\n",
    "\n",
    "lambda_ = 1\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument\n",
    "# (the neural network parameters)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# get the solution of the optimization\n",
    "nn_params_Final = res.x\n",
    "        \n",
    "# Obtain the optimal Theta1 and Theta2 back from nn_params\n",
    "Theta1_Final = np.reshape(nn_params_Final[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "Theta2_Final = np.reshape(nn_params_Final[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00927825 -0.00927825]\n",
      " [-0.00559136 -0.00559136]\n",
      " [-0.02017486 -0.02017486]\n",
      " [-0.00585433 -0.00585433]\n",
      " [ 0.00889912  0.00889912]\n",
      " [ 0.01315402  0.01315402]\n",
      " [-0.01049831 -0.01049831]\n",
      " [-0.01910997 -0.01910997]\n",
      " [-0.00836011 -0.00836011]\n",
      " [ 0.01976123  0.01976123]\n",
      " [ 0.00811587  0.00811587]\n",
      " [-0.01515689 -0.01515689]\n",
      " [ 0.00762814  0.00762814]\n",
      " [ 0.00827936  0.00827936]\n",
      " [ 0.02014747  0.02014747]\n",
      " [ 0.00315079  0.00315079]\n",
      " [-0.00674798 -0.00674798]\n",
      " [-0.0109273  -0.0109273 ]\n",
      " [ 0.01262954  0.01262954]\n",
      " [ 0.01809234  0.01809234]\n",
      " [ 0.31454497  0.31454497]\n",
      " [ 0.14895477  0.14895477]\n",
      " [ 0.17770766  0.17770766]\n",
      " [ 0.14745891  0.14745891]\n",
      " [ 0.15953087  0.15953087]\n",
      " [ 0.14381027  0.14381027]\n",
      " [ 0.11105659  0.11105659]\n",
      " [ 0.03839516  0.03839516]\n",
      " [ 0.0775739   0.0775739 ]\n",
      " [ 0.03592373  0.03592373]\n",
      " [ 0.07350885  0.07350885]\n",
      " [ 0.03392626  0.03392626]\n",
      " [ 0.0974007   0.0974007 ]\n",
      " [ 0.04486928  0.04486928]\n",
      " [ 0.05899539  0.05899539]\n",
      " [ 0.03843063  0.03843063]\n",
      " [ 0.06015138  0.06015138]\n",
      " [ 0.03153997  0.03153997]]\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 2.37486e-11\n",
      "\n",
      "\n",
      "Cost at (fixed) debugging parameters (w/ lambda = 1.000000): 0.511091 \n"
     ]
    }
   ],
   "source": [
    "utils.checkNNGradients(nnCostFunction, lambda_)\n",
    "\n",
    "# Also output the costFunction debugging values\n",
    "debug_J, _  = nnCostFunction(nn_params_Final, input_layer_size,\n",
    "                          hidden_layer_size, num_labels, X, y, lambda_)\n",
    "\n",
    "print('\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): %f ' % (lambda_, debug_J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Training Set Accuracy: 93.478261\n"
     ]
    }
   ],
   "source": [
    "pred = utils.predict(Theta1_Final, Theta2_Final, X)\n",
    "print(lambda_)\n",
    "print('Training Set Accuracy: %f' % (np.mean(pred == y) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Training Set Accuracy: 91.250000\n"
     ]
    }
   ],
   "source": [
    "pred = utils.predict(Theta1_Final, Theta2_Final, X_test)\n",
    "print(lambda_)\n",
    "print('Test Set Accuracy: %f' % (np.mean(pred == y_test) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
